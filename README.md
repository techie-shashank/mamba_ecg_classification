# Comparative Study of LSTM, MAMBA, PatchTST, Autoformer, and Hybrid Architectures for ECG Classification

This project presents a comparative study of LSTM, MAMBA, PatchTST, Autoformer, and hybrid (MAMBA + LSTM) architectures for ECG signal classification. The goal is to evaluate the performance, robustness, and representation learning capabilities of each model, particularly in handling complex time-series biomedical data.

## Project Presentation
ðŸ“„ [Link to presentation](https://drive.google.com/file/d/1Ax2YCpB9OaVNeMpD-qeII4mFn_9e05IC/view?usp=drive_link)

## Key Features

- **Multi-Model Support**: LSTM, Mamba, PatchTST, Autoformer, and Hybrid architectures for time series classification
- **Interpretability Tools**: Integrated Gradients analysis with multi-ECG batch processing
- **Experiment Management**: Comprehensive tracking with SQLite database and Streamlit dashboard
- **ECG Classification**: Specialized pipeline for PTB-XL dataset with 12-lead ECG analysis
- **Development Tools**: VS Code debugging configurations and organized project structure

## Quick Start

### 1. Environment Setup
```bash
# Create conda environment
conda create -n mamba_new python=3.10
conda activate mamba_new

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Complete Pipeline
```bash
# Quick start with default configuration
cd scripts
python quick_start.py
```

### 3. Model Training
```bash
# Train LSTM model
python src/train.py --model lstm --config configs/config.json

# Train Mamba model  
python src/train.py --model mamba --config configs/config.json

# Train PatchTST model
python src/train.py --model patchtst --config configs/config.json
```

### 4. Interpretability Analysis
```bash
# Single ECG analysis
cd scripts
python interpretability_analysis.py --ecg_id 12345 --model_path ../saved_model/ptbxl/

# Multi-ECG batch analysis
python interpretability_analysis.py --ecg_ids 12345,67890,11111 --model_path ../saved_model/ptbxl/
```

### 5. Experiment Dashboard
```bash
# Launch interactive dashboard
cd scripts
streamlit run experiment_dashboard.py
```

## Project Structure

```
â”œâ”€â”€ configs/           # Configuration files (config.json)
â”œâ”€â”€ data/             # Dataset storage (PTB-XL, etc.)
â”œâ”€â”€ outputs/          # Generated outputs
â”‚   â”œâ”€â”€ logs/         # All logging output
â”‚   â””â”€â”€ interpretability/  # Analysis results
â”œâ”€â”€ scripts/          # User-facing scripts
â”‚   â”œâ”€â”€ quick_start.py          # Complete pipeline runner
â”‚   â”œâ”€â”€ interpretability_analysis.py  # Multi-ECG analysis tool
â”‚   â”œâ”€â”€ experiment_dashboard.py # Streamlit dashboard
â”‚   â””â”€â”€ experiment_runner.py    # Hyperparameter search
â”œâ”€â”€ src/              # Core source code
â”‚   â”œâ”€â”€ data/         # Data loading and preprocessing
â”‚   â”œâ”€â”€ models/       # LSTM and Mamba architectures
â”‚   â”œâ”€â”€ evaluation/   # Metrics and interpretability
â”‚   â””â”€â”€ train.py      # Training pipeline
â””â”€â”€ .vscode/          # VS Code debugging configurations
```

## Development

### VS Code Integration
Pre-configured launch configurations for debugging:
- Train Model
- Test Model  
- Interpretability Analysis
- Quick Start Pipeline

### Logging
All logs automatically saved to `outputs/logs/` with timestamps and experiment tracking.

### Configuration Management
- Main config: `configs/config.json` (absolute paths)
- Legacy config: `config.json` (relative paths, for backward compatibility)

## Citation
If you use this codebase in your research, please cite appropriately.

## License
This project is licensed under the MIT License.



