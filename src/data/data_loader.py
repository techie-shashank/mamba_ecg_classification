
# ========== Standard Library Imports ==========
import os
import pickle
import hashlib
import json

# ========== Local Imports ==========
from data.ptbxl import load_and_prepare_ptbxl
from logger import logger


"""
Data Caching Mechanism
======================

This module implements an intelligent caching system to avoid duplicate data loading and preprocessing.

Key Features:
- Generates unique cache keys based on dataset name and relevant config parameters
- Caches only serializable data (arrays, metadata) - not PyTorch objects
- Recreates DataLoaders and Datasets from cached data when needed
- Provides significant performance improvements for repeated runs

Cache Key Generation:
- Based on dataset name and config parameters that affect data processing
- Uses MD5 hash to ensure uniqueness and manageable file names
- Only includes relevant config parameters (sampling_rate, limit, etc.)

Cache Storage:
- Stored in data/cache/ directory as pickle files
- Automatically creates cache directory if it doesn't exist
- Files named using the cache key for easy identification

Performance Impact:
- First call: Loads, preprocesses, and caches data (~0.5-1.0s for small datasets)
- Subsequent calls: Loads from cache and recreates DataLoaders (~0.01s)
- Typical speedup: 50-100x for cached loads
"""


def get_cache_key(dataset_name, config):
    """
    Generate a unique cache key based on dataset name and relevant config parameters.
    Args:
        dataset_name (str): Name of the dataset.
        config (dict): Configuration dictionary.
    Returns:
        str: Unique cache key.
    """
    # Only include config parameters that affect data processing
    relevant_config = {
        'sampling_rate': config.get('sampling_rate'),
        'limit': config.get('limit'),
        'is_multilabel': config.get('is_multilabel'),
        'batch_size': config.get('batch_size')
    }
    
    cache_string = f"{dataset_name}_{json.dumps(relevant_config, sort_keys=True)}"
    return hashlib.md5(cache_string.encode()).hexdigest()


def get_cache_dir():
    """Get the cache directory for preprocessed data."""
    cache_dir = os.path.join(os.getcwd(), "data", "cache")
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir


def clear_cache():
    """
    Clear all cached preprocessed data.
    Useful when config changes or data updates require fresh preprocessing.
    """
    cache_dir = get_cache_dir()
    if os.path.exists(cache_dir):
        import shutil
        shutil.rmtree(cache_dir)
        logger.info("üóëÔ∏è Cache cleared - all preprocessed data removed")
        print("üóëÔ∏è Cache cleared - next data load will process from scratch")
    else:
        logger.info("üóëÔ∏è Cache directory doesn't exist - nothing to clear")
        print("üóëÔ∏è No cache to clear")


def get_cache_info():
    """
    Get information about cached files.
    Returns:
        dict: Cache statistics and file information.
    """
    cache_dir = get_cache_dir()
    cache_files = []
    total_size = 0
    
    if os.path.exists(cache_dir):
        for filename in os.listdir(cache_dir):
            if filename.endswith('.pkl'):
                filepath = os.path.join(cache_dir, filename)
                file_size = os.path.getsize(filepath)
                cache_files.append({
                    'filename': filename,
                    'size_mb': file_size / (1024 * 1024),
                    'modified': os.path.getmtime(filepath)
                })
                total_size += file_size
    
    return {
        'num_files': len(cache_files),
        'total_size_mb': total_size / (1024 * 1024),
        'files': cache_files
    }


def save_preprocessed_data(cache_key, data):
    """
    Save preprocessed data to cache.
    Args:
        cache_key (str): Unique cache key.
        data (tuple): Data to cache.
    """
    cache_dir = get_cache_dir()
    cache_file = os.path.join(cache_dir, f"{cache_key}.pkl")
    
    logger.info(f"üíæ Saving preprocessed data to cache: {cache_file}")
    with open(cache_file, 'wb') as f:
        pickle.dump(data, f)
    logger.info("‚úÖ Preprocessed data saved to cache")


def load_preprocessed_data(cache_key):
    """
    Load preprocessed data from cache.
    Args:
        cache_key (str): Unique cache key.
    Returns:
        tuple: Cached data or None if not found.
    """
    cache_dir = get_cache_dir()
    cache_file = os.path.join(cache_dir, f"{cache_key}.pkl")
    
    if os.path.exists(cache_file):
        logger.info(f"üìÇ Loading preprocessed data from cache: {cache_file}")
        with open(cache_file, 'rb') as f:
            data = pickle.load(f)
        logger.info("‚úÖ Preprocessed data loaded from cache")
        return data
    return None


def load_and_prepare(dataset_name, config):
    """
    Load, preprocess, split, and prepare the dataset with caching.
    Args:
        dataset_name (str): Name of the dataset.
        config (dict): Configuration dictionary.
    Returns:
        tuple: (data_loaders, datasets, data_arrays, metadata, annotation_dfs)
    """
    # Generate cache key
    cache_key = get_cache_key(dataset_name, config)
    logger.info(f"üîë Cache key: {cache_key}")
    
    # Try to load from cache first
    cached_data = load_preprocessed_data(cache_key)
    if cached_data is not None:
        logger.info("üöÄ Using cached preprocessed data - recreating DataLoaders")
        
        # Unpack cached data (only data arrays, metadata, and annotation_dfs are cached)
        data_arrays, metadata, annotation_dfs = cached_data
        
        # Recreate datasets and data loaders from cached arrays
        data_loaders, datasets = create_datasets_and_loaders(data_arrays, config)
        
        logger.info("‚úÖ DataLoaders recreated from cached data")
        return data_loaders, datasets, data_arrays, metadata, annotation_dfs
    
    # Cache miss - load and preprocess data
    logger.info("‚ùå Cache miss - loading and preprocessing data from scratch")
    
    if dataset_name == "ptbxl":
        data_loaders, datasets, data_arrays, metadata, annotation_dfs = load_and_prepare_ptbxl(config)
    else:
        raise ValueError(f"Unsupported dataset: {dataset_name}")
    
    # Save only serializable data to cache (exclude DataLoaders and Datasets)
    cacheable_data = (data_arrays, metadata, annotation_dfs)
    save_preprocessed_data(cache_key, cacheable_data)
    
    return data_loaders, datasets, data_arrays, metadata, annotation_dfs


def create_datasets_and_loaders(data_arrays, config):
    """
    Create PyTorch datasets and data loaders from data arrays.
    Args:
        data_arrays (dict): Dictionary containing train/val/test data arrays.
        config (dict): Configuration dictionary.
    Returns:
        tuple: (data_loaders, datasets)
    """
    from data.ptbxl import PTBXL
    from torch.utils.data import DataLoader
    
    # Extract data arrays
    X_train, y_train = data_arrays['train']
    X_val, y_val = data_arrays['val'] 
    X_test, y_test = data_arrays['test']
    
    # Create datasets
    train_dataset = PTBXL(X_train, y_train)
    val_dataset = PTBXL(X_val, y_val)
    test_dataset = PTBXL(X_test, y_test)
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=config["batch_size"], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config["batch_size"], shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=config["batch_size"], shuffle=False)
    
    data_loaders = {"train": train_loader, "val": val_loader, "test": test_loader}
    datasets = {"train": train_dataset, "val": val_dataset, "test": test_dataset}
    
    return data_loaders, datasets
